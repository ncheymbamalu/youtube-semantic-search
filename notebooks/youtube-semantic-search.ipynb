{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[0m\n",
      "├── \u001b[00mLICENSE\u001b[0m\n",
      "├── \u001b[00mMakefile\u001b[0m\n",
      "├── \u001b[00mREADME.md\u001b[0m\n",
      "├── \u001b[01;34martifacts\u001b[0m\n",
      "│   └── \u001b[01;34membedding_model\u001b[0m\n",
      "│       ├── \u001b[01;34m1_Pooling\u001b[0m\n",
      "│       │   └── \u001b[00mconfig.json\u001b[0m\n",
      "│       ├── \u001b[00mREADME.md\u001b[0m\n",
      "│       ├── \u001b[00mconfig.json\u001b[0m\n",
      "│       ├── \u001b[00mconfig_sentence_transformers.json\u001b[0m\n",
      "│       ├── \u001b[00mmodel.safetensors\u001b[0m\n",
      "│       ├── \u001b[00mmodules.json\u001b[0m\n",
      "│       ├── \u001b[00msentence_bert_config.json\u001b[0m\n",
      "│       ├── \u001b[00mspecial_tokens_map.json\u001b[0m\n",
      "│       ├── \u001b[00mtokenizer.json\u001b[0m\n",
      "│       ├── \u001b[00mtokenizer_config.json\u001b[0m\n",
      "│       └── \u001b[00mvocab.txt\u001b[0m\n",
      "├── \u001b[00mconfig.yaml\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "│   └── \u001b[00myoutube_transcripts.parquet\u001b[0m\n",
      "├── \u001b[00mdata.dvc\u001b[0m\n",
      "├── \u001b[01;34mdocker\u001b[0m\n",
      "├── \u001b[01;34mnotebooks\u001b[0m\n",
      "│   └── \u001b[00myoutube-semantic-search.ipynb\u001b[0m\n",
      "├── \u001b[00mpoetry.lock\u001b[0m\n",
      "├── \u001b[00mpyproject.toml\u001b[0m\n",
      "└── \u001b[01;34msrc\u001b[0m\n",
      "    ├── \u001b[00m__init__.py\u001b[0m\n",
      "    ├── \u001b[00mapp.py\u001b[0m\n",
      "    ├── \u001b[00metl.py\u001b[0m\n",
      "    ├── \u001b[00mfrontend.py\u001b[0m\n",
      "    ├── \u001b[00mlogger.py\u001b[0m\n",
      "    ├── \u001b[00mpaths.py\u001b[0m\n",
      "    ├── \u001b[00msemantic_search.py\u001b[0m\n",
      "    └── \u001b[00mutils.py\u001b[0m\n",
      "\n",
      "8 directories, 28 files\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Dependencies`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from pathlib import PosixPath\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.paths import PathConfig, load_config\n",
    "from src.utils import embed_transcripts, transcribe_videos\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<polars.config.Config at 0x31967b940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the Pandas display options\n",
    "pd.set_option(\"display.max_rows\", 100) # max number of rows to display\n",
    "pd.set_option(\"display.max_columns\", None) # max number of columns to display\n",
    "\n",
    "# set the Polars display options\n",
    "pl.Config(\n",
    "    tbl_rows=10, # max number of rows to display\n",
    "    tbl_cols=100, # max number of columns to display\n",
    "    tbl_width_chars=1000, # max table width, in characters\n",
    "    fmt_str_lengths=50, # max number of characters to display for a pl.Utf8 (str) dtype column\n",
    "    fmt_table_cell_list_len=20 # max number of items to display for a pl.List dtype column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`extract-transform-load`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the YouTube channel IDs\n",
    "youtube_channel_ids: list[str] = load_config().get(\"youtube_channel_ids\")\n",
    "youtube_channel_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of pl.LazyFrames, one per YouTube channel ID\n",
    "lfs: list[pl.LazyFrame] = Parallel(n_jobs=-1)(\n",
    "    delayed(transcribe_videos)(channel_id) for channel_id in tqdm(youtube_channel_ids)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertically concatenate the list of pl.LazyFrames into a single pl.DataFrame\n",
    "df: pl.DataFrame = (\n",
    "    pl.concat(lfs, how=\"vertical\")\n",
    "    .unique(subset=\"video_id\")\n",
    "    .sort(by=\"creation_date\")\n",
    "    .collect()\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [**`Hugging Face Embedding Models Leaderboard`**](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "- [**`15 Best Open Source Text Embedding Models`**](https://www.graft.com/blog/open-source-text-embedding-models#15-open-source-text-embedding-models-updated-april-2024)\n",
    "\n",
    "- [**`Alibaba-NLP/gte-large-en-v1.5 Embedding Model`**](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the embedding model from Hugging Face\n",
    "model_id: str = load_config().get(\"embedding_model_id\")\n",
    "embedding_model: SentenceTransformer = SentenceTransformer(model_id, trust_remote_code=True)\n",
    "embedding_model, embedding_model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding model locally to ~/artifacts/embedding_model/\n",
    "artifacts_dir: PosixPath = PathConfig.ARTIFACTS_DIR\n",
    "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "embedding_model.save(str(artifacts_dir / \"embedding_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create serialized embeddings for the video transcripts, and ...\n",
    "# write the resulting pl.DataFrame to ~/data/youtube_transcripts.parquet\n",
    "(\n",
    "    df\n",
    "    .pipe(embed_transcripts, embedding_model)\n",
    "    .with_columns(pl.col(\"creation_date\").str.to_datetime())\n",
    "    .write_parquet(PathConfig.PROCESSED_DATA_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poetry run python src/etl.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 131.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.00s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# update ~/data/youtube_transcripts.parquet\n",
    "cd ..\n",
    "make etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_315, 5)\n",
      "┌─────────────┬─────────────────────┬─────────────────────────────────────────────────────┬─────────────────────────────────────────────────────┬─────────────────────────────────────────────────────┐\n",
      "│ video_id    ┆ creation_date       ┆ title                                               ┆ transcript                                          ┆ embedding                                           │\n",
      "│ ---         ┆ ---                 ┆ ---                                                 ┆ ---                                                 ┆ ---                                                 │\n",
      "│ str         ┆ datetime[μs]        ┆ str                                                 ┆ str                                                 ┆ binary                                              │\n",
      "╞═════════════╪═════════════════════╪═════════════════════════════════════════════════════╪═════════════════════════════════════════════════════╪═════════════════════════════════════════════════════╡\n",
      "│ wYPUhge9w5c ┆ 2018-12-23 01:38:06 ┆ Linear Regression: A friendly introduction          ┆ hi I'm Louis Serrano and this is a friendly introd… ┆ b\"Z\\xff\\xa1\\xbe\\xd8g\\x8d\\xbd\\xe7\\xc9H\\xbf\\xb8\\xd6\\… │\n",
      "│ jbluHIgBmBo ┆ 2019-01-01 20:08:19 ┆ Logistic Regression and the Perceptron Algorithm: … ┆ hi I'm Louis serrano and this is a friendly introd… ┆ b\"\\x9a\\xe2\\x0d>\\xac\\xb5\\xff>6r\\xbd=\\x0a\\xf8\\xd6=5\\… │\n",
      "│ jbluHIgBmBo ┆ 2019-01-01 20:08:19 ┆ Logistic Regression and the Perceptron Algorithm: … ┆ hi I'm Louis serrano and this is a friendly introd… ┆ b\"\\x1eT;?f\\x08\\x04?<\\xa6\\xa6><f\\x07>:\\x04%>\\xb4\\xb… │\n",
      "│ Lpr__X8zuE8 ┆ 2019-01-27 14:19:19 ┆ Support Vector Machines (SVMs): A friendly introdu… ┆ hello my name is luis serrano and this is a friend… ┆ b\"L\\xd0P?\\xc0&F<$\\xcdA=\\xf8\\x88r<\\x82\\x11\\xa3\\xbd\\… │\n",
      "│ QXOkPvFM6NU ┆ 2019-01-28 06:20:05 ┆ Clustering: K-means and Hierarchical                ┆ hello i'm luis serrano and this video is about flu… ┆ b\"\\x85\\xf0\\xe0>fz\\xc7>h\\x94\\x0b\\xbdD\\xd8\\x01\\xbd\\x… │\n",
      "│ …           ┆ …                   ┆ …                                                   ┆ …                                                   ┆ …                                                   │\n",
      "│ QxlvD_yEgz8 ┆ 2024-10-01 19:00:15 ┆ How NVIDIA GPUs Accelerate Your Python Workflow     ┆ what is going on guys welcome back in this video t… ┆ b\"$\\x0c^>\\xba\\x0a\\x82?l\\xe0\\x99\\xbeB2\\xe6>\\x96\\x1c… │\n",
      "│ yOLdYoEo5Vc ┆ 2024-10-01 23:00:31 ┆ Build Better RAGs with Contextual Retrieval         ┆ hey everyone my name is Vin and in this video we'r… ┆ b\"\\x08\\xda\\x83<\\xc2\\x1fo\\xbe\\xcc\\xdd\\xa0>\\x80\\xf6h… │\n",
      "│ TNUlsAXH2Qc ┆ 2024-10-02 04:44:43 ┆ Solving Massively Parallel Job Scheduling with Fly… ┆ all right looks like we are live that's great welc… ┆ b\"\\x0a\\x9c*\\xbf2L\\x1e\\xbe\\xd7\\x81\\xc7\\xbe\\x98\\xa1e… │\n",
      "│ _1dS6ddf4uU ┆ 2024-10-02 07:55:10 ┆ OpenAI DevDay 2024 - What No One is Talking About!  ┆ okay so open yesterday had their Dev day and they … ┆ b\"\\xee\\xb3I>A\\xae\\x8e\\xbf\\x18\\x95\\xae>\\x15\\xcbH?\\x… │\n",
      "│ f-JNZXGY9Kw ┆ 2024-10-02 15:20:29 ┆ From spending 7 hours on Gaming to cracking Intern… ┆ I was in my uh first year of my engineering and uh… ┆ b\"\\xc6\\x81p\\xbf\\x80*\\x99\\xbe\\xeb\\xff\\x1d\\xbf\\x07xC… │\n",
      "└─────────────┴─────────────────────┴─────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# read in ~/data/youtube_transcripts.parquet \n",
    "print(pl.read_parquet(PathConfig.PROCESSED_DATA_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
