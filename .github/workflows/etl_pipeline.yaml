name: etl-pipeline

on:
  workflow_dispatch: # allows the pipeline to be executed manually from GitHub
  
  schedule:
    - cron: '0 0 * * *' # pipeline will run every day at midnight

env:
  POETRY_URL: https://install.python-poetry.org
  POETRY_VERSION: 1.8.3

jobs:
  etl_pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repo content
        uses: actions/checkout@v4

      - name: Cache Poetry cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/pypoetry
          key: poetry-cache-${{ runner.os }}-3.10-${{ env.POETRY_VERSION }} # 3.10 is Python version

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Poetry
        run: |
          curl -sSL ${{ env.POETRY_URL }} | python - --version ${{ env.POETRY_VERSION }}
          echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: poetry install

      - name: Set up DVC
        uses: iterative/setup-dvc@v1

      - name: Pull data with DVC
        env:
          GDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}
        run: |
          dvc pull

      - name: Execute pipeline
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          TOKENIZERS_PARALLELISM: ${{ secrets.TOKENIZERS_PARALLELISM }}
        run: make etl

      - name: Push data to remote
        env:
          GDRIVE_CREDENTIALS_DATA: ${{ secrets.GDRIVE_CREDENTIALS_DATA }}
        run: |
          dvc add ./data
          git add data.dvc
          git commit -m "updating data and pushing to remote"
          dvc push
          git push
